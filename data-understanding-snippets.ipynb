{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb7adc2",
   "metadata": {
    "papermill": {
     "duration": 0.001869,
     "end_time": "2025-03-17T01:22:36.289299",
     "exception": false,
     "start_time": "2025-03-17T01:22:36.287430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the \"Data Understanding\" phase, you explore and understand your dataset. This includes checking for missing values, duplicates, inconsistencies, data types, and general data quality. Below is a series of Python scripts using Pandas that you can use for this exploration, fixing, and cleaning process before moving to the analysis phase.\n",
    "\n",
    "### 1. **Load Data and Basic Exploration**\n",
    "First, load the dataset and get a sense of its general structure.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Get basic information about the dataset (data types, non-null counts)\n",
    "print(df.info())\n",
    "\n",
    "# Summary statistics of numerical columns\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(df.duplicated().sum())\n",
    "```\n",
    "\n",
    "### 2. **Handling Missing Values**\n",
    "Depending on your analysis needs, missing values can be handled by either filling them with mean/median values, using a forward or backward fill, or dropping them entirely.\n",
    "\n",
    "```python\n",
    "# Drop rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Or fill missing values with a specific value (mean, median, etc.)\n",
    "df['column_name'] = df['column_name'].fillna(df['column_name'].mean())  # Example for numerical columns\n",
    "\n",
    "# For categorical columns, you might want to fill with the mode\n",
    "df['categorical_column'] = df['categorical_column'].fillna(df['categorical_column'].mode()[0])\n",
    "\n",
    "# Forward fill (use the previous value to fill missing values)\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Backward fill (use the next value to fill missing values)\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "```\n",
    "\n",
    "### 3. **Removing Duplicates**\n",
    "Check and remove duplicate rows if necessary.\n",
    "\n",
    "```python\n",
    "# Check for duplicates\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "```\n",
    "\n",
    "### 4. **Handling Inconsistent Data**\n",
    "Data inconsistency often arises in categorical columns. It's essential to ensure that all values in these columns follow a standard format.\n",
    "\n",
    "```python\n",
    "# Check unique values in a categorical column\n",
    "print(df['categorical_column'].unique())\n",
    "\n",
    "# Standardize categorical column values (e.g., lowercase all values)\n",
    "df['categorical_column'] = df['categorical_column'].str.lower()\n",
    "\n",
    "# Replace specific inconsistent values\n",
    "df['categorical_column'] = df['categorical_column'].replace({'old_value': 'new_value'})\n",
    "```\n",
    "\n",
    "### 5. **Handling Outliers**\n",
    "Outliers can skew the analysis. Use the IQR (Interquartile Range) method or z-scores to detect and handle them.\n",
    "\n",
    "```python\n",
    "# Using IQR to detect outliers in numerical columns\n",
    "Q1 = df['numerical_column'].quantile(0.25)\n",
    "Q3 = df['numerical_column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out rows where the values are outliers\n",
    "df_no_outliers = df[(df['numerical_column'] >= (Q1 - 1.5 * IQR)) & (df['numerical_column'] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "# Alternatively, handle outliers by capping them (replace values outside the IQR range with the boundary values)\n",
    "df['numerical_column'] = df['numerical_column'].clip(lower=Q1 - 1.5 * IQR, upper=Q3 + 1.5 * IQR)\n",
    "```\n",
    "\n",
    "### 6. **Convert Data Types**\n",
    "Ensure that columns are of the correct data type (e.g., converting date columns to datetime, category columns to `category`).\n",
    "\n",
    "```python\n",
    "# Convert date columns to datetime format\n",
    "df['date_column'] = pd.to_datetime(df['date_column'])\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "df['categorical_column'] = df['categorical_column'].astype('category')\n",
    "\n",
    "# Convert numerical columns to appropriate data types (e.g., float)\n",
    "df['numerical_column'] = df['numerical_column'].astype(float)\n",
    "```\n",
    "\n",
    "### 7. **Feature Engineering (if needed)**\n",
    "You may want to create new features based on existing ones, such as extracting year, month, or day from a datetime column.\n",
    "\n",
    "```python\n",
    "# Extract year, month, and day from a datetime column\n",
    "df['year'] = df['date_column'].dt.year\n",
    "df['month'] = df['date_column'].dt.month\n",
    "df['day'] = df['date_column'].dt.day\n",
    "\n",
    "# Create a new feature from numerical columns (e.g., interaction between columns)\n",
    "df['new_feature'] = df['numerical_column_1'] * df['numerical_column_2']\n",
    "```\n",
    "\n",
    "### 8. **Normalize or Scale Features (if needed for analysis)**\n",
    "Depending on the algorithm you're planning to use, you may need to scale or normalize features.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Normalize numerical columns (scaling to [0, 1])\n",
    "scaler = MinMaxScaler()\n",
    "df[['numerical_column_1', 'numerical_column_2']] = scaler.fit_transform(df[['numerical_column_1', 'numerical_column_2']])\n",
    "\n",
    "# Standardize numerical columns (mean = 0, std = 1)\n",
    "scaler = StandardScaler()\n",
    "df[['numerical_column_1', 'numerical_column_2']] = scaler.fit_transform(df[['numerical_column_1', 'numerical_column_2']])\n",
    "```\n",
    "\n",
    "### 9. **Final Check**\n",
    "After all the cleaning and transformation, perform a final check on the dataset.\n",
    "\n",
    "```python\n",
    "# Final check of the dataset\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.head())\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for any remaining duplicates\n",
    "print(df.duplicated().sum())\n",
    "```\n",
    "\n",
    "By following this set of steps, you should be able to prepare your dataset for the analysis phase. You may need to adapt some of the steps depending on your specific dataset and the types of issues it may have."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.016333,
   "end_time": "2025-03-17T01:22:36.712930",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-17T01:22:33.696597",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
